{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zEZSQoeg4CL",
        "outputId": "175a08d3-f0fc-40b3-d9a9-74e38e978cfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "# Define map elements\n",
        "EMPTY = 0\n",
        "WALL = 1\n",
        "LAVA = 2\n",
        "TREASURE = 3\n",
        "EXIT = 4\n",
        "START = 5\n",
        "\n",
        "# Define Colors for Visualization\n",
        "COLOR_MAP = {\n",
        "    EMPTY: \"white\",\n",
        "    WALL: \"brown\",\n",
        "    LAVA: \"red\",\n",
        "    TREASURE: \"yellow\",\n",
        "    EXIT: \"green\",\n",
        "    START: \"blue\",\n",
        "}\n",
        "\n",
        "# Experience replay buffer\n",
        "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])"
      ],
      "metadata": {
        "id": "YN7mmJwdENDD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=5000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append(Experience(state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        experiences = random.sample(self.buffer, k=batch_size)\n",
        "        states = torch.stack([torch.tensor(e.state, dtype=torch.float) for e in experiences])\n",
        "        actions = torch.tensor([e.action for e in experiences])\n",
        "        rewards = torch.tensor([e.reward for e in experiences], dtype=torch.float)\n",
        "        next_states = torch.stack([torch.tensor(e.next_state, dtype=torch.float) for e in experiences])\n",
        "        dones = torch.tensor([e.done for e in experiences], dtype=torch.float)\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "class DQNModel(nn.Module):\n",
        "    \"\"\"Lightweight Q-Network\"\"\"\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DQNModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ],
      "metadata": {
        "id": "W0uoMw7qEYpM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "H56s7G70ZnYM"
      },
      "outputs": [],
      "source": [
        "class DungeonAgent:\n",
        "    \"\"\"Agent that learns to generate levels\"\"\"\n",
        "    def __init__(self, state_size, action_size, seed=42):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "        self.qnetwork = DQNModel(state_size, action_size)\n",
        "        self.optimizer = optim.Adam(self.qnetwork.parameters(), lr=0.001)\n",
        "        self.memory = ReplayBuffer(5000)\n",
        "\n",
        "        self.batch_size = 32\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_decay = 0.999995\n",
        "        self.epsilon_min = 0.000001\n",
        "        print(self.epsilon_decay)\n",
        "        print(self.epsilon_min)\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"Epsilon-greedy action selection\"\"\"\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.action_size - 1)\n",
        "        state = torch.tensor(state, dtype=torch.float).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            return torch.argmax(self.qnetwork(state)).item()\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Save experience & train\"\"\"\n",
        "        self.memory.push(state, action, reward, next_state, done)\n",
        "        if len(self.memory) > self.batch_size:\n",
        "            self.learn(self.memory.sample(self.batch_size))\n",
        "\n",
        "    def learn(self, experiences):\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        Q_expected = self.qnetwork(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "        Q_targets = rewards + (self.gamma * self.qnetwork(next_states).max(1)[0] * (1 - dones))\n",
        "\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "\n",
        "class DungeonEnvironment:\n",
        "    \"\"\"Environment for RL-based level generation with maze-like structures\"\"\"\n",
        "    def __init__(self, width, height):\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.grid = np.zeros((height, width), dtype=np.int8)\n",
        "        self.max_steps = width * height // 2  # Increased steps for better structures\n",
        "        self.current_step = 0\n",
        "        self.element_types = [EMPTY, WALL, LAVA, TREASURE]\n",
        "        self.start_pos = None\n",
        "        self.exit_pos = None\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset dungeon with randomized start and exit positions\"\"\"\n",
        "        self.grid.fill(EMPTY)\n",
        "\n",
        "        # Set boundary walls (DO NOT MODIFY THESE)\n",
        "        self.grid[0, :] = WALL\n",
        "        self.grid[:, 0] = WALL\n",
        "        self.grid[-1, :] = WALL\n",
        "        self.grid[:, -1] = WALL\n",
        "\n",
        "        # Randomly place start and exit\n",
        "        self.start_pos, self.exit_pos = self._place_start_and_exit()\n",
        "        self.grid[self.start_pos[1], self.start_pos[0]] = START\n",
        "        self.grid[self.exit_pos[1], self.exit_pos[0]] = EXIT\n",
        "\n",
        "        self.current_step = 0\n",
        "        return self._get_state()\n",
        "\n",
        "    def _place_start_and_exit(self):\n",
        "        \"\"\"Randomly place start and exit, ensuring they are far apart\"\"\"\n",
        "        while True:\n",
        "            start_x, start_y = random.randint(1, self.width - 2), random.randint(1, self.height - 2)\n",
        "            exit_x, exit_y = random.randint(1, self.width - 2), random.randint(1, self.height - 2)\n",
        "\n",
        "            # Ensure they are far apart\n",
        "            min_distance = max(self.width, self.height) // 2\n",
        "            if abs(start_x - exit_x) + abs(start_y - exit_y) >= min_distance:\n",
        "                return (start_x, start_y), (exit_x, exit_y)\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Modify grid based on action\"\"\"\n",
        "        self.current_step += 1\n",
        "        position_idx = action // len(self.element_types)\n",
        "        element_type = self.element_types[action % len(self.element_types)]\n",
        "\n",
        "        y, x = divmod(position_idx, self.width)\n",
        "        reward = 0\n",
        "        done = False\n",
        "\n",
        "        # Restrict modifications to the interior (NO MODIFICATION of walls on boundary)\n",
        "        if x == 0 or y == 0 or x == self.width-1 or y == self.height-1:\n",
        "            return self._get_state(), -5, False  # Penalize modifying boundaries\n",
        "\n",
        "        if (x, y) == self.start_pos or (x, y) == self.exit_pos:\n",
        "            reward -= 5  # Can't modify start/exit\n",
        "        else:\n",
        "            self.grid[y, x] = element_type\n",
        "\n",
        "            # **Improved Reward System for Maze-Like Walls**\n",
        "            if element_type == WALL:\n",
        "                if self._creates_dead_end(x, y):\n",
        "                    reward -= 2  # Penalize dead ends\n",
        "                else:\n",
        "                    reward += 10  # Strongly encourage walls\n",
        "\n",
        "                if self._too_many_walls():\n",
        "                    reward -= 4  # Penalize excessive walls\n",
        "\n",
        "                if self._wall_continues_maze(x, y):\n",
        "                    reward += 10  # Reward walls that extend the maze structure\n",
        "\n",
        "            elif element_type == LAVA:\n",
        "                if abs(x - self.start_pos[0]) + abs(y - self.start_pos[1]) < 3:\n",
        "                    reward -= 3  # Penalize lava near the start\n",
        "                else:\n",
        "                    reward += 10  # Reward strategic lava placement\n",
        "\n",
        "            elif element_type == TREASURE:\n",
        "                if self._is_clustered_treasure(x, y):\n",
        "                    reward -= 1  # Penalize clustering treasures\n",
        "                else:\n",
        "                    reward += 5  # Reward scattered treasure placement\n",
        "\n",
        "        # **Bonus Reward for Path Complexity**\n",
        "        if self.current_step >= self.max_steps:\n",
        "            done = True\n",
        "            path_length = self._calculate_path_length()\n",
        "            reward += path_length * 0.3  # Encourage longer paths\n",
        "\n",
        "        return self._get_state(), reward, done\n",
        "\n",
        "    def _creates_dead_end(self, x, y):\n",
        "        \"\"\"Check if placing a wall at (x,y) creates a dead end\"\"\"\n",
        "        open_paths = 0\n",
        "        for dx, dy in [(0, -1), (1, 0), (0, 1), (-1, 0)]:\n",
        "            nx, ny = x + dx, y + dy\n",
        "            if 0 <= nx < self.width and 0 <= ny < self.height and self.grid[ny, nx] != WALL:\n",
        "                open_paths += 1\n",
        "        return open_paths <= 1  # If only 1 way out, it's a dead end\n",
        "\n",
        "    def _too_many_walls(self):\n",
        "        \"\"\"Check if the map has too many walls (avoid over-blocking)\"\"\"\n",
        "        wall_count = np.sum(self.grid == WALL)\n",
        "        total_cells = self.width * self.height\n",
        "        return wall_count > total_cells * 0.7  # Limit walls to 70% of the map\n",
        "\n",
        "    def _wall_continues_maze(self, x, y):\n",
        "        \"\"\"Encourage walls that form a maze-like pattern\"\"\"\n",
        "        adjacent_walls = 0\n",
        "        for dx, dy in [(0, -1), (1, 0), (0, 1), (-1, 0)]:\n",
        "            nx, ny = x + dx, y + dy\n",
        "            if 0 <= nx < self.width and 0 <= ny < self.height and self.grid[ny, nx] == WALL:\n",
        "                adjacent_walls += 1\n",
        "        return adjacent_walls >= 2  # Encourage walls that connect to existing ones\n",
        "\n",
        "    def _is_clustered_treasure(self, x, y):\n",
        "        \"\"\"Check if treasure is too close to another treasure\"\"\"\n",
        "        for dy in [-1, 0, 1]:\n",
        "            for dx in [-1, 0, 1]:\n",
        "                if 0 <= x + dx < self.width and 0 <= y + dy < self.height:\n",
        "                    if self.grid[y + dy, x + dx] == TREASURE:\n",
        "                        return True\n",
        "        return False\n",
        "\n",
        "    def _calculate_path_length(self):\n",
        "        \"\"\"Estimate the shortest path from start to exit\"\"\"\n",
        "        queue = [(self.start_pos, 0)]\n",
        "        visited = set()\n",
        "        visited.add(self.start_pos)\n",
        "\n",
        "        while queue:\n",
        "            (x, y), dist = queue.pop(0)\n",
        "\n",
        "            if (x, y) == self.exit_pos:\n",
        "                return dist  # Shortest path found\n",
        "\n",
        "            # Check all four directions\n",
        "            for dx, dy in [(0, -1), (1, 0), (0, 1), (-1, 0)]:\n",
        "                nx, ny = x + dx, y + dy\n",
        "                if (0 <= nx < self.width and 0 <= ny < self.height and\n",
        "                    (nx, ny) not in visited and self.grid[ny, nx] != WALL):\n",
        "                    queue.append(((nx, ny), dist + 1))\n",
        "                    visited.add((nx, ny))\n",
        "\n",
        "        return 0  # No path found\n",
        "\n",
        "    def _get_state(self):\n",
        "        return self.grid.flatten().astype(np.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dungeon_generator(width, height, num_episodes=500):\n",
        "    \"\"\"Train RL model to generate levels\"\"\"\n",
        "    env = DungeonEnvironment(width, height)\n",
        "    state_size = width * height\n",
        "    action_size = width * height * len(env.element_types)\n",
        "    agent = DungeonAgent(state_size, action_size)\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "\n",
        "        if episode % 100 == 0:\n",
        "            print(f\"Episode {episode}, Epsilon: {agent.epsilon:.3f}\")\n",
        "\n",
        "    return agent, env\n",
        "\n",
        "def generate_dungeon_with_model(agent, width, height):\n",
        "    \"\"\"Generate a level using a trained RL model\"\"\"\n",
        "    env = DungeonEnvironment(width, height)\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = agent.act(state)\n",
        "        state, _, done = env.step(action)\n",
        "\n",
        "    return env.grid\n",
        "\n",
        "def visualize_dungeon(grid):\n",
        "    \"\"\"Display dungeon grid using matplotlib\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    for y in range(grid.shape[0]):\n",
        "        for x in range(grid.shape[1]):\n",
        "            rect = plt.Rectangle((x, grid.shape[0] - y - 1), 1, 1, facecolor=COLOR_MAP[grid[y, x]], edgecolor='black')\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "    ax.set_xlim(0, grid.shape[1])\n",
        "    ax.set_ylim(0, grid.shape[0])\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "NPm7LAwFEnPR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and Visualize\n",
        "if __name__ == \"__main__\":\n",
        "    trained_agent, trained_env = train_dungeon_generator(20, 20, num_episodes=1000)\n",
        "    generated_dungeon = generate_dungeon_with_model(trained_agent, 20, 20)\n",
        "    visualize_dungeon(generated_dungeon)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CA9KajnmEoQm",
        "outputId": "347941b0-502c-4fbc-d95b-3656f7761174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.999995\n",
            "1e-06\n",
            "Episode 0, Epsilon: 0.999\n",
            "Episode 100, Epsilon: 0.904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fByapT5CcOtd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "e0dd7200-95f5-4fa9-deda-28419caabd7e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'generate_dungeon_with_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6c0b534dd6e3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerated_dungeon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_dungeon_with_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvisualize_dungeon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_dungeon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'generate_dungeon_with_model' is not defined"
          ]
        }
      ],
      "source": [
        "generated_dungeon = generate_dungeon_with_model(trained_agent, 20, 20)\n",
        "visualize_dungeon(generated_dungeon)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def save_model(agent, filename=\"dungeon_rl_model.pth\"):\n",
        "    \"\"\"Save trained DQN model weights.\"\"\"\n",
        "    torch.save(agent.qnetwork.state_dict(), filename)\n",
        "    print(f\"Model saved as {filename}\")\n",
        "\n",
        "def load_model(agent, filename=\"dungeon_rl_model.pth\"):\n",
        "    \"\"\"Load trained DQN model weights.\"\"\"\n",
        "    agent.qnetwork.load_state_dict(torch.load(filename))\n",
        "    agent.qnetwork.eval()  # Set to evaluation mode\n",
        "    print(f\"Model loaded from {filename}\")\n"
      ],
      "metadata": {
        "id": "LlMwIjVdWmUs"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_model(trained_agent)"
      ],
      "metadata": {
        "id": "QHarvzJvWnZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model before generating dungeons\n",
        "agent = DungeonAgent(state_size=10*10, action_size=10*10*4)  # Ensure same architecture\n",
        "load_model(agent)\n",
        "\n",
        "# Generate dungeon using the loaded model\n",
        "dungeon = generate_dungeon_with_model(agent, 10, 10)\n",
        "visualize_dungeon(dungeon)"
      ],
      "metadata": {
        "id": "rTRpKz_FWuAo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}